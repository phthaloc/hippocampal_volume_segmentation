{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains code that will kick off training and testing processes\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from experiments.UNetExperiment import UNetExperiment\n",
    "from data_prep.HippocampusDatasetLoader import LoadHippocampusData\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Holds configuration parameters\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Defines important parameters:\n",
    "        name: name of algorithm (neural network)\n",
    "        root_dir: \n",
    "        n_epochs: number of epochs to train\n",
    "        learning_rate: initial learning rate\n",
    "        batch_size: size of image/label batches to feed the neural network at training at once\n",
    "        patch_size:\n",
    "        test_results_dir: \n",
    "        \"\"\"\n",
    "        self.name = 'Basic_unet'\n",
    "        self.root_dir = r'../data/TrainingSet/'\n",
    "        self.n_epochs = 1\n",
    "        self.learning_rate = 0.0002\n",
    "        self.batch_size = 8\n",
    "        self.patch_size = 64\n",
    "        self.test_results_dir = '../results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed generator:\n",
    "np.random.seed(seed=5)\n",
    "\n",
    "# define relative training, validation and testing size:\n",
    "train_size = 0.8\n",
    "valid_size = 0.1\n",
    "test_size = 0.1\n",
    "\n",
    "# Get configuration\n",
    "\n",
    "# TASK: Fill in parameters of the Config class and specify directory where the data is stored and \n",
    "# directory where results will go\n",
    "c = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Processed 260 files, total 9198 slices\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# TASK: LoadHippocampusData is not complete. Go to the implementation and complete it. \n",
    "data = LoadHippocampusData(root_dir=c.root_dir, y_shape=c.patch_size,\n",
    "                           z_shape=c.patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hippocampus_176.nii.gz'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 64, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test-train-val split\n",
    "# In a real world scenario you would probably do multiple splits for \n",
    "# multi-fold training to improve your model quality\n",
    "\n",
    "keys = range(len(data))\n",
    "\n",
    "# Here, random permutation of keys array would be useful in case if we do something like \n",
    "# a k-fold training and combining the results. \n",
    "keys = np.array(keys)\n",
    "np.random.shuffle(keys)\n",
    "\n",
    "split = dict()\n",
    "\n",
    "# TASK: create three keys in the dictionary: \"train\", \"val\" and \"test\". In each key, store\n",
    "# the array with indices of training volumes to be used for training, validation \n",
    "# and testing respectively.\n",
    "# <YOUR CODE GOES HERE>\n",
    "split['train'] = keys[:int(train_size*len(keys))]\n",
    "split['val'] = keys[int(train_size*len(keys)):int((train_size+valid_size)*len(keys))]\n",
    "split['test'] = keys[int((train_size+valid_size)*len(keys)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30, 175,  65, 203, 205,  44,  27,  80, 253, 181, 113, 143, 204,\n",
       "         7, 208, 158, 112, 155, 190, 231, 228,   8,  73, 118, 189, 206])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (model): UnetSkipConnectionBlock(\n",
      "    (model): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      )\n",
      "      (2): UnetSkipConnectionBlock(\n",
      "        (model): Sequential(\n",
      "          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (1): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "          (3): UnetSkipConnectionBlock(\n",
      "            (model): Sequential(\n",
      "              (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "              (1): Sequential(\n",
      "                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              )\n",
      "              (2): Sequential(\n",
      "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              )\n",
      "              (3): UnetSkipConnectionBlock(\n",
      "                (model): Sequential(\n",
      "                  (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "                  (1): Sequential(\n",
      "                    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                  )\n",
      "                  (2): Sequential(\n",
      "                    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                  )\n",
      "                  (3): UnetSkipConnectionBlock(\n",
      "                    (model): Sequential(\n",
      "                      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "                      (1): Sequential(\n",
      "                        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                        (1): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                      )\n",
      "                      (2): Sequential(\n",
      "                        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                        (1): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                      )\n",
      "                      (3): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "                    )\n",
      "                  )\n",
      "                  (4): Sequential(\n",
      "                    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                  )\n",
      "                  (5): Sequential(\n",
      "                    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                  )\n",
      "                  (6): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "                )\n",
      "              )\n",
      "              (4): Sequential(\n",
      "                (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              )\n",
      "              (5): Sequential(\n",
      "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              )\n",
      "              (6): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "            )\n",
      "          )\n",
      "          (4): Sequential(\n",
      "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "          (5): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "          (6): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      )\n",
      "      (5): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]             640\n",
      "    InstanceNorm2d-2           [-1, 64, 64, 64]               0\n",
      "         LeakyReLU-3           [-1, 64, 64, 64]               0\n",
      "            Conv2d-4           [-1, 64, 64, 64]          36,928\n",
      "    InstanceNorm2d-5           [-1, 64, 64, 64]               0\n",
      "         LeakyReLU-6           [-1, 64, 64, 64]               0\n",
      "         MaxPool2d-7           [-1, 64, 32, 32]               0\n",
      "            Conv2d-8          [-1, 128, 32, 32]          73,856\n",
      "    InstanceNorm2d-9          [-1, 128, 32, 32]               0\n",
      "        LeakyReLU-10          [-1, 128, 32, 32]               0\n",
      "           Conv2d-11          [-1, 128, 32, 32]         147,584\n",
      "   InstanceNorm2d-12          [-1, 128, 32, 32]               0\n",
      "        LeakyReLU-13          [-1, 128, 32, 32]               0\n",
      "        MaxPool2d-14          [-1, 128, 16, 16]               0\n",
      "           Conv2d-15          [-1, 256, 16, 16]         295,168\n",
      "   InstanceNorm2d-16          [-1, 256, 16, 16]               0\n",
      "        LeakyReLU-17          [-1, 256, 16, 16]               0\n",
      "           Conv2d-18          [-1, 256, 16, 16]         590,080\n",
      "   InstanceNorm2d-19          [-1, 256, 16, 16]               0\n",
      "        LeakyReLU-20          [-1, 256, 16, 16]               0\n",
      "        MaxPool2d-21            [-1, 256, 8, 8]               0\n",
      "           Conv2d-22            [-1, 512, 8, 8]       1,180,160\n",
      "   InstanceNorm2d-23            [-1, 512, 8, 8]               0\n",
      "        LeakyReLU-24            [-1, 512, 8, 8]               0\n",
      "           Conv2d-25            [-1, 512, 8, 8]       2,359,808\n",
      "   InstanceNorm2d-26            [-1, 512, 8, 8]               0\n",
      "        LeakyReLU-27            [-1, 512, 8, 8]               0\n",
      "        MaxPool2d-28            [-1, 512, 4, 4]               0\n",
      "           Conv2d-29           [-1, 1024, 4, 4]       4,719,616\n",
      "   InstanceNorm2d-30           [-1, 1024, 4, 4]               0\n",
      "        LeakyReLU-31           [-1, 1024, 4, 4]               0\n",
      "           Conv2d-32           [-1, 1024, 4, 4]       9,438,208\n",
      "   InstanceNorm2d-33           [-1, 1024, 4, 4]               0\n",
      "        LeakyReLU-34           [-1, 1024, 4, 4]               0\n",
      "  ConvTranspose2d-35            [-1, 512, 8, 8]       2,097,664\n",
      "UnetSkipConnectionBlock-36           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-37            [-1, 512, 8, 8]       4,719,104\n",
      "        LeakyReLU-38            [-1, 512, 8, 8]               0\n",
      "           Conv2d-39            [-1, 512, 8, 8]       2,359,808\n",
      "        LeakyReLU-40            [-1, 512, 8, 8]               0\n",
      "  ConvTranspose2d-41          [-1, 256, 16, 16]         524,544\n",
      "UnetSkipConnectionBlock-42          [-1, 512, 16, 16]               0\n",
      "           Conv2d-43          [-1, 256, 16, 16]       1,179,904\n",
      "        LeakyReLU-44          [-1, 256, 16, 16]               0\n",
      "           Conv2d-45          [-1, 256, 16, 16]         590,080\n",
      "        LeakyReLU-46          [-1, 256, 16, 16]               0\n",
      "  ConvTranspose2d-47          [-1, 128, 32, 32]         131,200\n",
      "UnetSkipConnectionBlock-48          [-1, 256, 32, 32]               0\n",
      "           Conv2d-49          [-1, 128, 32, 32]         295,040\n",
      "        LeakyReLU-50          [-1, 128, 32, 32]               0\n",
      "           Conv2d-51          [-1, 128, 32, 32]         147,584\n",
      "        LeakyReLU-52          [-1, 128, 32, 32]               0\n",
      "  ConvTranspose2d-53           [-1, 64, 64, 64]          32,832\n",
      "UnetSkipConnectionBlock-54          [-1, 128, 64, 64]               0\n",
      "           Conv2d-55           [-1, 64, 64, 64]          73,792\n",
      "        LeakyReLU-56           [-1, 64, 64, 64]               0\n",
      "           Conv2d-57           [-1, 64, 64, 64]          36,928\n",
      "        LeakyReLU-58           [-1, 64, 64, 64]               0\n",
      "           Conv2d-59            [-1, 3, 64, 64]             195\n",
      "UnetSkipConnectionBlock-60            [-1, 3, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 31,030,723\n",
      "Trainable params: 31,030,723\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 50.62\n",
      "Params size (MB): 118.37\n",
      "Estimated Total Size (MB): 169.01\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Set up and run experiment\n",
    "\n",
    "# TASK: Class UNetExperiment has missing pieces. Go to the file and fill them in\n",
    "exp = UNetExperiment(config=c, split=split, dataset=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment started.\n",
      "Training epoch 0...\n",
      "\n",
      "Epoch: 0 Train loss: 1.1220207214355469, 0.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.13933327794075012, 1.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.12006347626447678, 2.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.06404244899749756, 3.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.04624509438872337, 4.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.0584537647664547, 5.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.03408191725611687, 6.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.06991492956876755, 7.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.056971561163663864, 8.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.05170554667711258, 9.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.017450911924242973, 11.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.050478920340538025, 12.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.021281659603118896, 13.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.053723275661468506, 14.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.031062819063663483, 15.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.015659961849451065, 16.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.02456829883158207, 17.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.04069700837135315, 18.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.02806789055466652, 19.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.030578169971704483, 20.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.023259487003087997, 21.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.053914740681648254, 23.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.023711811751127243, 24.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.028677554801106453, 25.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.02959536761045456, 26.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.02918180823326111, 27.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.020321669057011604, 28.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01467035710811615, 29.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.019652195274829865, 30.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01616804115474224, 31.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.026676218956708908, 32.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.019660709425807, 33.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.027699965983629227, 35.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.031017381697893143, 36.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.009121953509747982, 37.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.00788821466267109, 38.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.0152976019307971, 39.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.023578274995088577, 40.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.022775985300540924, 41.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.012028063647449017, 42.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.021658344194293022, 43.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.013418011367321014, 44.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.019911926239728928, 45.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.017045674845576286, 47.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.026893533766269684, 48.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.014686000533401966, 49.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.017641989514231682, 50.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.02217811346054077, 51.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.021560750901699066, 52.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.012791917659342289, 53.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01776457577943802, 54.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.012618412263691425, 55.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.015555924735963345, 56.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.017983902245759964, 57.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.013941909186542034, 59.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.02889011614024639, 60.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.017789460718631744, 61.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.013205613940954208, 62.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.0200306698679924, 63.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.021999292075634003, 64.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01486650574952364, 65.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.02361002191901207, 66.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.012771748006343842, 67.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.029346805065870285, 68.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.017758680507540703, 69.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.007239309139549732, 71.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.010367907583713531, 72.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.014311961829662323, 73.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.021326079964637756, 74.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.02084917575120926, 75.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.019136615097522736, 76.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.017165010794997215, 77.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.018422771245241165, 78.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01526198536157608, 79.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.009208924137055874, 80.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.012933903373777866, 81.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.009164692834019661, 83.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.009302767924964428, 84.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.011894533410668373, 85.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.015517918393015862, 86.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01715882122516632, 87.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.014415564946830273, 88.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01416809856891632, 89.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.013397682458162308, 90.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.011111192405223846, 91.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.019642729312181473, 92.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.017940551042556763, 93.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.010222986340522766, 95.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01571262814104557, 96.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.013110476545989513, 97.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01445552334189415, 98.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.018106695264577866, 99.3% complete\n",
      ".......\n",
      "Training complete\n",
      "Validating epoch 0...\n",
      "Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013133516535162926\n",
      "Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01119502168148756\n",
      "Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010933183133602142\n",
      "Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005206478759646416\n",
      "Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 0.016137059777975082\n",
      "Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01582241803407669\n",
      "Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00512206694111228\n",
      "Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013219539076089859\n",
      "Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007969441823661327\n",
      "Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011067681014537811\n",
      "Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012880773283541203\n",
      "Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015345495194196701\n",
      "Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008128753863275051\n",
      "Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014379194006323814\n",
      "Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01624119095504284\n",
      "Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015245774760842323\n",
      "Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 0.016559002920985222\n",
      "Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 0.017117749899625778\n",
      "Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01279113907366991\n",
      "Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012137031182646751\n",
      "Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013352828100323677\n",
      "Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0186958909034729\n",
      "Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011960201896727085\n",
      "Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01624244451522827\n",
      "Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014917686581611633\n",
      "Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 0.019186804071068764\n",
      "Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 0.023933593183755875\n",
      "Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 0.018240753561258316\n",
      "Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 0.018273858353495598\n",
      "Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 0.022950416430830956\n",
      "Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014580560848116875\n",
      "Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01508520357310772\n",
      "Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012773316353559494\n",
      "Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 0.024778390303254128\n",
      "Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009679595939815044\n",
      "Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 0.02171444147825241\n",
      "Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 0.025451550260186195\n",
      "Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 0.019378652796149254\n",
      "Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01948043704032898\n",
      "Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 0.02632959373295307\n",
      "Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010669578798115253\n",
      "Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 0.02239902690052986\n",
      "Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014007141813635826\n",
      "Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 0.023459583520889282\n",
      "Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011192148551344872\n",
      "Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0017393999733030796\n",
      "Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 0.016815247014164925\n",
      "Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010918552055954933\n",
      "Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011698140762746334\n",
      "Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005091176368296146\n",
      "Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 0.016105547547340393\n",
      "Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0098081324249506\n",
      "Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010293262079358101\n",
      "Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011866746470332146\n",
      "Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 0.020928366109728813\n",
      "Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014662910252809525\n",
      "Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00989362969994545\n",
      "Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 0.016460362821817398\n",
      "Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.016566898673772812\n",
      "Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015798138454556465\n",
      "Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013100751675665379\n",
      "Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011368317529559135\n",
      "Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012759342789649963\n",
      "Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012045982293784618\n",
      "Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0183903519064188\n",
      "Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012287001125514507\n",
      "Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 0.021643053740262985\n",
      "Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 0.019412118941545486\n",
      "Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014387635514140129\n",
      "Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015488196164369583\n",
      "Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00994177721440792\n",
      "Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013902408070862293\n",
      "Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 0.022751694545149803\n",
      "Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01795751228928566\n",
      "Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014163108542561531\n",
      "Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 0.016329187899827957\n",
      "Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 0.019856218248605728\n",
      "Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012311629951000214\n",
      "Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01646341383457184\n",
      "Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012356581166386604\n",
      "Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011594787240028381\n",
      "Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 0.02101144753396511\n",
      "Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 0.017957227304577827\n",
      "Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011867926456034184\n",
      "Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010932075791060925\n",
      "Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 0.017705999314785004\n",
      "Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01518560666590929\n",
      "Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014086508192121983\n",
      "Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0171083714812994\n",
      "Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011942402459681034\n",
      "Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005696892272680998\n",
      "Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00567948492243886\n",
      "Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 0.020585695281624794\n",
      "Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01558096706867218\n",
      "Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01713741384446621\n",
      "Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012539693154394627\n",
      "Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01707354374229908\n",
      "Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012299378402531147\n",
      "Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014990195631980896\n",
      "Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 0.016417918726801872\n",
      "Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007913642562925816\n",
      "Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01626579277217388\n",
      "Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008477365598082542\n",
      "Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01926475018262863\n",
      "Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 0.019865501672029495\n",
      "Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01581939309835434\n",
      "Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015117825008928776\n",
      "Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 0.016310669481754303\n",
      "Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006622586864978075\n",
      "Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01838330738246441\n",
      "Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013080242089927197\n",
      "Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 0.018008742481470108\n",
      "Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01815856620669365\n",
      "Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013215908780694008\n",
      "Batch 114. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014493943192064762\n",
      "Batch 115. Data shape torch.Size([5, 1, 64, 64]) Loss 0.01815151423215866\n",
      "Validation complete\n",
      "Run complete. Total time: 00:01:33\n"
     ]
    }
   ],
   "source": [
    "# You could free up memory by deleting the dataset\n",
    "# as it has been copied into loaders\n",
    "# del dataset \n",
    "\n",
    "# run training\n",
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "hippocampus_101.nii.gz Dice 0.9122. 3.85% complete\n",
      "hippocampus_146.nii.gz Dice 0.9131. 7.69% complete\n",
      "hippocampus_017.nii.gz Dice 0.8923. 11.54% complete\n",
      "hippocampus_156.nii.gz Dice 0.8929. 15.38% complete\n",
      "hippocampus_065.nii.gz Dice 0.9267. 19.23% complete\n",
      "hippocampus_162.nii.gz Dice 0.9130. 23.08% complete\n",
      "hippocampus_367.nii.gz Dice 0.9016. 26.92% complete\n",
      "hippocampus_036.nii.gz Dice 0.8893. 30.77% complete\n",
      "hippocampus_220.nii.gz Dice 0.8534. 34.62% complete\n",
      "hippocampus_171.nii.gz Dice 0.9060. 38.46% complete\n",
      "hippocampus_316.nii.gz Dice 0.8586. 42.31% complete\n",
      "hippocampus_037.nii.gz Dice 0.8502. 46.15% complete\n",
      "hippocampus_025.nii.gz Dice 0.8725. 50.00% complete\n",
      "hippocampus_223.nii.gz Dice 0.9032. 53.85% complete\n",
      "hippocampus_203.nii.gz Dice 0.9004. 57.69% complete\n",
      "hippocampus_056.nii.gz Dice 0.8107. 61.54% complete\n",
      "hippocampus_130.nii.gz Dice 0.8848. 65.38% complete\n",
      "hippocampus_125.nii.gz Dice 0.8136. 69.23% complete\n",
      "hippocampus_301.nii.gz Dice 0.8463. 73.08% complete\n",
      "hippocampus_217.nii.gz Dice 0.8157. 76.92% complete\n",
      "hippocampus_199.nii.gz Dice 0.6611. 80.77% complete\n",
      "hippocampus_350.nii.gz Dice 0.8546. 84.62% complete\n",
      "hippocampus_227.nii.gz Dice 0.9083. 88.46% complete\n",
      "hippocampus_083.nii.gz Dice 0.9028. 92.31% complete\n",
      "hippocampus_341.nii.gz Dice 0.8636. 96.15% complete\n",
      "hippocampus_330.nii.gz Dice 0.7802. 100.00% complete\n",
      "\n",
      "Testing complete.\n"
     ]
    }
   ],
   "source": [
    "# prep and run testing\n",
    "\n",
    "# TASK: Test method is not complete. Go to the method and complete it\n",
    "results_json = exp.run_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results_json[\"config\"] = vars(c)\n",
    "\n",
    "with open(os.path.join(exp.out_dir, 'results.json'), 'w') as out_file:\n",
    "    json.dump(results_json, out_file, indent=2, separators=(',', ': '))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38pytorch",
   "language": "python",
   "name": "py38pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
