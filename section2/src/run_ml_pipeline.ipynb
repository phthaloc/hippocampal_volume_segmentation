{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains code that will kick off training and testing processes\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from experiments.UNetExperiment import UNetExperiment\n",
    "from data_prep.HippocampusDatasetLoader import LoadHippocampusData\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Holds configuration parameters\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Defines important parameters:\n",
    "        name: name of algorithm (neural network)\n",
    "        root_dir: \n",
    "        n_epochs: number of epochs to train\n",
    "        learning_rate: initial learning rate\n",
    "        batch_size: size of image/label batches to feed the neural network at training at once\n",
    "        patch_size:\n",
    "        test_results_dir: \n",
    "        \"\"\"\n",
    "        self.name = 'Basic_unet'\n",
    "#         self.root_dir = r'../data/TrainingSet/'\n",
    "        self.root_dir = r'../../section1/data/TrainingSet/'\n",
    "        self.n_epochs = 1\n",
    "        self.learning_rate = 0.0002\n",
    "        self.batch_size = 8\n",
    "        self.patch_size = 64\n",
    "        self.test_results_dir = '../results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed generator:\n",
    "np.random.seed(seed=5)\n",
    "\n",
    "# define relative training, validation and testing size:\n",
    "train_size = 0.8\n",
    "valid_size = 0.1\n",
    "test_size = 0.1\n",
    "\n",
    "# Get configuration\n",
    "\n",
    "# TASK: Fill in parameters of the Config class and specify directory where the data is stored and \n",
    "# directory where results will go\n",
    "c = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Processed 260 files, total 9198 slices\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# TASK: LoadHippocampusData is not complete. Go to the implementation and complete it. \n",
    "data = LoadHippocampusData(root_dir=c.root_dir, y_shape=c.patch_size,\n",
    "                           z_shape=c.patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hippocampus_325.nii.gz'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 64, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test-train-val split\n",
    "# In a real world scenario you would probably do multiple splits for \n",
    "# multi-fold training to improve your model quality\n",
    "\n",
    "keys = range(len(data))\n",
    "\n",
    "# Here, random permutation of keys array would be useful in case if we do something like \n",
    "# a k-fold training and combining the results. \n",
    "keys = np.array(keys)\n",
    "np.random.shuffle(keys)\n",
    "\n",
    "split = dict()\n",
    "\n",
    "# TASK: create three keys in the dictionary: \"train\", \"val\" and \"test\". In each key, store\n",
    "# the array with indices of training volumes to be used for training, validation \n",
    "# and testing respectively.\n",
    "# <YOUR CODE GOES HERE>\n",
    "split['train'] = keys[:int(train_size*len(keys))]\n",
    "split['val'] = keys[int(train_size*len(keys)):int((train_size+valid_size)*len(keys))]\n",
    "split['test'] = keys[int((train_size+valid_size)*len(keys)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30, 175,  65, 203, 205,  44,  27,  80, 253, 181, 113, 143, 204,\n",
       "         7, 208, 158, 112, 155, 190, 231, 228,   8,  73, 118, 189, 206])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (model): UnetSkipConnectionBlock(\n",
      "    (model): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      )\n",
      "      (2): UnetSkipConnectionBlock(\n",
      "        (model): Sequential(\n",
      "          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (1): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "          (3): UnetSkipConnectionBlock(\n",
      "            (model): Sequential(\n",
      "              (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "              (1): Sequential(\n",
      "                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              )\n",
      "              (2): Sequential(\n",
      "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              )\n",
      "              (3): UnetSkipConnectionBlock(\n",
      "                (model): Sequential(\n",
      "                  (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "                  (1): Sequential(\n",
      "                    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                  )\n",
      "                  (2): Sequential(\n",
      "                    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                  )\n",
      "                  (3): UnetSkipConnectionBlock(\n",
      "                    (model): Sequential(\n",
      "                      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "                      (1): Sequential(\n",
      "                        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                        (1): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                      )\n",
      "                      (2): Sequential(\n",
      "                        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                        (1): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                      )\n",
      "                      (3): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "                    )\n",
      "                  )\n",
      "                  (4): Sequential(\n",
      "                    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                  )\n",
      "                  (5): Sequential(\n",
      "                    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                  )\n",
      "                  (6): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "                )\n",
      "              )\n",
      "              (4): Sequential(\n",
      "                (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              )\n",
      "              (5): Sequential(\n",
      "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              )\n",
      "              (6): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "            )\n",
      "          )\n",
      "          (4): Sequential(\n",
      "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "          (5): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "          (6): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      )\n",
      "      (5): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]             640\n",
      "    InstanceNorm2d-2           [-1, 64, 64, 64]               0\n",
      "         LeakyReLU-3           [-1, 64, 64, 64]               0\n",
      "            Conv2d-4           [-1, 64, 64, 64]          36,928\n",
      "    InstanceNorm2d-5           [-1, 64, 64, 64]               0\n",
      "         LeakyReLU-6           [-1, 64, 64, 64]               0\n",
      "         MaxPool2d-7           [-1, 64, 32, 32]               0\n",
      "            Conv2d-8          [-1, 128, 32, 32]          73,856\n",
      "    InstanceNorm2d-9          [-1, 128, 32, 32]               0\n",
      "        LeakyReLU-10          [-1, 128, 32, 32]               0\n",
      "           Conv2d-11          [-1, 128, 32, 32]         147,584\n",
      "   InstanceNorm2d-12          [-1, 128, 32, 32]               0\n",
      "        LeakyReLU-13          [-1, 128, 32, 32]               0\n",
      "        MaxPool2d-14          [-1, 128, 16, 16]               0\n",
      "           Conv2d-15          [-1, 256, 16, 16]         295,168\n",
      "   InstanceNorm2d-16          [-1, 256, 16, 16]               0\n",
      "        LeakyReLU-17          [-1, 256, 16, 16]               0\n",
      "           Conv2d-18          [-1, 256, 16, 16]         590,080\n",
      "   InstanceNorm2d-19          [-1, 256, 16, 16]               0\n",
      "        LeakyReLU-20          [-1, 256, 16, 16]               0\n",
      "        MaxPool2d-21            [-1, 256, 8, 8]               0\n",
      "           Conv2d-22            [-1, 512, 8, 8]       1,180,160\n",
      "   InstanceNorm2d-23            [-1, 512, 8, 8]               0\n",
      "        LeakyReLU-24            [-1, 512, 8, 8]               0\n",
      "           Conv2d-25            [-1, 512, 8, 8]       2,359,808\n",
      "   InstanceNorm2d-26            [-1, 512, 8, 8]               0\n",
      "        LeakyReLU-27            [-1, 512, 8, 8]               0\n",
      "        MaxPool2d-28            [-1, 512, 4, 4]               0\n",
      "           Conv2d-29           [-1, 1024, 4, 4]       4,719,616\n",
      "   InstanceNorm2d-30           [-1, 1024, 4, 4]               0\n",
      "        LeakyReLU-31           [-1, 1024, 4, 4]               0\n",
      "           Conv2d-32           [-1, 1024, 4, 4]       9,438,208\n",
      "   InstanceNorm2d-33           [-1, 1024, 4, 4]               0\n",
      "        LeakyReLU-34           [-1, 1024, 4, 4]               0\n",
      "  ConvTranspose2d-35            [-1, 512, 8, 8]       2,097,664\n",
      "UnetSkipConnectionBlock-36           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-37            [-1, 512, 8, 8]       4,719,104\n",
      "        LeakyReLU-38            [-1, 512, 8, 8]               0\n",
      "           Conv2d-39            [-1, 512, 8, 8]       2,359,808\n",
      "        LeakyReLU-40            [-1, 512, 8, 8]               0\n",
      "  ConvTranspose2d-41          [-1, 256, 16, 16]         524,544\n",
      "UnetSkipConnectionBlock-42          [-1, 512, 16, 16]               0\n",
      "           Conv2d-43          [-1, 256, 16, 16]       1,179,904\n",
      "        LeakyReLU-44          [-1, 256, 16, 16]               0\n",
      "           Conv2d-45          [-1, 256, 16, 16]         590,080\n",
      "        LeakyReLU-46          [-1, 256, 16, 16]               0\n",
      "  ConvTranspose2d-47          [-1, 128, 32, 32]         131,200\n",
      "UnetSkipConnectionBlock-48          [-1, 256, 32, 32]               0\n",
      "           Conv2d-49          [-1, 128, 32, 32]         295,040\n",
      "        LeakyReLU-50          [-1, 128, 32, 32]               0\n",
      "           Conv2d-51          [-1, 128, 32, 32]         147,584\n",
      "        LeakyReLU-52          [-1, 128, 32, 32]               0\n",
      "  ConvTranspose2d-53           [-1, 64, 64, 64]          32,832\n",
      "UnetSkipConnectionBlock-54          [-1, 128, 64, 64]               0\n",
      "           Conv2d-55           [-1, 64, 64, 64]          73,792\n",
      "        LeakyReLU-56           [-1, 64, 64, 64]               0\n",
      "           Conv2d-57           [-1, 64, 64, 64]          36,928\n",
      "        LeakyReLU-58           [-1, 64, 64, 64]               0\n",
      "           Conv2d-59            [-1, 3, 64, 64]             195\n",
      "UnetSkipConnectionBlock-60            [-1, 3, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 31,030,723\n",
      "Trainable params: 31,030,723\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 50.62\n",
      "Params size (MB): 118.37\n",
      "Estimated Total Size (MB): 169.01\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Set up and run experiment\n",
    "\n",
    "# TASK: Class UNetExperiment has missing pieces. Go to the file and fill them in\n",
    "exp = UNetExperiment(config=c, split=split, dataset=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment started.\n",
      "Training epoch 0...\n",
      "\n",
      "Epoch: 0 Train loss: 1.0656324625015259, 0.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.21671271324157715, 1.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.11152403801679611, 2.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.0656108632683754, 3.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.06775471568107605, 4.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.051191531121730804, 5.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.04403594508767128, 6.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.05291362106800079, 7.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.05446762964129448, 8.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.050918325781822205, 9.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.045103538781404495, 11.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.04885540530085564, 12.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.040518056601285934, 13.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.03443560004234314, 14.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.03621306270360947, 15.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.025178570300340652, 16.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.011168076656758785, 17.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.030299872159957886, 18.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.0320923775434494, 19.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.0387895405292511, 20.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.035577744245529175, 21.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.035224881023168564, 23.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.03783677890896797, 24.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.03161453828215599, 25.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.022611776366829872, 26.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.02639775350689888, 27.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.012702508829534054, 28.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01355847530066967, 29.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.027201635763049126, 30.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.020763922482728958, 31.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.020838968455791473, 32.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.018595896661281586, 33.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.03278634324669838, 35.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.021934451535344124, 36.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.022156495600938797, 37.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.015571899712085724, 38.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.021066971123218536, 39.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.019745761528611183, 40.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.017916088923811913, 41.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.018643371760845184, 42.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.012942974455654621, 43.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01518210582435131, 44.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.02574295736849308, 45.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.0240534245967865, 47.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.012183827348053455, 48.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.012822114862501621, 49.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.014115364290773869, 50.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.014128505252301693, 51.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.004717385396361351, 52.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.011110851541161537, 53.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.008259509690105915, 54.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01716802455484867, 55.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.017607655376195908, 56.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01941692642867565, 57.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.015485490672290325, 59.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.018405739217996597, 60.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.023671571165323257, 61.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.008859418332576752, 62.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.011740674264729023, 63.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.009852070361375809, 64.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.016526231542229652, 65.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.013523663394153118, 66.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.016365671530365944, 67.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.009473721496760845, 68.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.014894161373376846, 69.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.017658594995737076, 71.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01368790864944458, 72.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.018302205950021744, 73.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.008017424494028091, 74.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.016650576144456863, 75.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.012972677126526833, 76.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01767873391509056, 77.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.009362167678773403, 78.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.02479935809969902, 79.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.025974834337830544, 80.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.007766589056700468, 81.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.022352328523993492, 83.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01334285270422697, 84.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.015209178440272808, 85.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.013611644506454468, 86.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.02220120094716549, 87.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.011862950399518013, 88.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.014678587205708027, 89.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.013833066448569298, 90.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.0180364977568388, 91.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.013562542386353016, 92.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.014284525066614151, 93.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.018885567784309387, 95.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.011645834892988205, 96.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.011401481926441193, 97.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.023127319291234016, 98.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.02209641970694065, 99.3% complete\n",
      ".......\n",
      "Training complete\n",
      "Validating epoch 0...\n",
      "Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015115557238459587\n",
      "Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013793173246085644\n",
      "Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010586484335362911\n",
      "Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 0.016901059076189995\n",
      "Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 0.022045575082302094\n",
      "Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 0.017551949247717857\n",
      "Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01988701894879341\n",
      "Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.019464991986751556\n",
      "Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 0.017637375742197037\n",
      "Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 0.017858896404504776\n",
      "Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009917042218148708\n",
      "Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 0.02063143253326416\n",
      "Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 0.02248973585665226\n",
      "Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009801404550671577\n",
      "Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.018395908176898956\n",
      "Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 0.020062655210494995\n",
      "Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015234516002237797\n",
      "Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0040502469055354595\n",
      "Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01570168137550354\n",
      "Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013124043121933937\n",
      "Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 0.02129361964762211\n",
      "Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011536987498402596\n",
      "Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 0.017852041870355606\n",
      "Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012925420887768269\n",
      "Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0198651310056448\n",
      "Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011586869135499\n",
      "Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011597971431910992\n",
      "Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01857483573257923\n",
      "Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012102792970836163\n",
      "Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014269637875258923\n",
      "Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 0.018316831439733505\n",
      "Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010934898629784584\n",
      "Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013156292028725147\n",
      "Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 0.018419397994875908\n",
      "Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0056443423964083195\n",
      "Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01629415526986122\n",
      "Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 0.019013725221157074\n",
      "Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 0.031510595232248306\n",
      "Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008227033540606499\n",
      "Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01105030719190836\n",
      "Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014795098453760147\n",
      "Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 0.02327657863497734\n",
      "Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01476707961410284\n",
      "Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009911914356052876\n",
      "Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01604986935853958\n",
      "Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015660597011446953\n",
      "Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009981782175600529\n",
      "Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010198893956840038\n",
      "Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012364059686660767\n",
      "Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014977243728935719\n",
      "Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007796345744282007\n",
      "Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 0.02202150598168373\n",
      "Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011668327264487743\n",
      "Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014080028049647808\n",
      "Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01260329782962799\n",
      "Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008590913377702236\n",
      "Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012452371418476105\n",
      "Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01729545369744301\n",
      "Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.022015471011400223\n",
      "Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013684423640370369\n",
      "Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014874251559376717\n",
      "Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01042493898421526\n",
      "Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 0.019707990810275078\n",
      "Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00916297733783722\n",
      "Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 0.026888571679592133\n",
      "Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01789952628314495\n",
      "Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013668609783053398\n",
      "Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005536896642297506\n",
      "Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011113310232758522\n",
      "Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014721900224685669\n",
      "Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.020749418064951897\n",
      "Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01350532565265894\n",
      "Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015857774764299393\n",
      "Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011704904958605766\n",
      "Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 0.02232986129820347\n",
      "Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 0.02011692151427269\n",
      "Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015671778470277786\n",
      "Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011868163011968136\n",
      "Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 0.018564753234386444\n",
      "Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012575171887874603\n",
      "Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01717044785618782\n",
      "Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015685340389609337\n",
      "Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015960507094860077\n",
      "Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 0.017357073724269867\n",
      "Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012611625716090202\n",
      "Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015857314690947533\n",
      "Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 0.016319284215569496\n",
      "Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004223857540637255\n",
      "Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01176069863140583\n",
      "Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 0.016926899552345276\n",
      "Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008718110620975494\n",
      "Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015051886439323425\n",
      "Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 0.016424385830760002\n",
      "Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006422278005629778\n",
      "Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 0.023670975118875504\n",
      "Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 0.022846661508083344\n",
      "Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 0.032931260764598846\n",
      "Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011749239638447762\n",
      "Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 0.028586797416210175\n",
      "Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007762748748064041\n",
      "Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014333025552332401\n",
      "Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01625809259712696\n",
      "Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 0.016040997579693794\n",
      "Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009216086938977242\n",
      "Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014168147929012775\n",
      "Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013357364572584629\n",
      "Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012891885824501514\n",
      "Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01873602904379368\n",
      "Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 0.018970919772982597\n",
      "Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01139548048377037\n",
      "Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 0.024209043011069298\n",
      "Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006566278636455536\n",
      "Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013677907176315784\n",
      "Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 0.027723221108317375\n",
      "Batch 114. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015553351491689682\n",
      "Batch 115. Data shape torch.Size([5, 1, 64, 64]) Loss 0.008749818429350853\n",
      "Validation complete\n",
      "Run complete. Total time: 00:01:33\n"
     ]
    }
   ],
   "source": [
    "# You could free up memory by deleting the dataset\n",
    "# as it has been copied into loaders\n",
    "# del dataset \n",
    "\n",
    "# run training\n",
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "hippocampus_101.nii.gz Dice 0.8563, Jaccard 0.7487. 3.85% complete\n",
      "hippocampus_146.nii.gz Dice 0.9215, Jaccard 0.8545. 7.69% complete\n",
      "hippocampus_017.nii.gz Dice 0.9049, Jaccard 0.8264. 11.54% complete\n",
      "hippocampus_156.nii.gz Dice 0.8914, Jaccard 0.8041. 15.38% complete\n",
      "hippocampus_065.nii.gz Dice 0.9147, Jaccard 0.8428. 19.23% complete\n",
      "hippocampus_162.nii.gz Dice 0.9176, Jaccard 0.8478. 23.08% complete\n",
      "hippocampus_367.nii.gz Dice 0.8877, Jaccard 0.7981. 26.92% complete\n",
      "hippocampus_036.nii.gz Dice 0.8985, Jaccard 0.8157. 30.77% complete\n",
      "hippocampus_220.nii.gz Dice 0.9093, Jaccard 0.8337. 34.62% complete\n",
      "hippocampus_171.nii.gz Dice 0.8842, Jaccard 0.7925. 38.46% complete\n",
      "hippocampus_316.nii.gz Dice 0.8676, Jaccard 0.7661. 42.31% complete\n",
      "hippocampus_037.nii.gz Dice 0.8351, Jaccard 0.7170. 46.15% complete\n",
      "hippocampus_025.nii.gz Dice 0.8784, Jaccard 0.7831. 50.00% complete\n",
      "hippocampus_223.nii.gz Dice 0.8959, Jaccard 0.8115. 53.85% complete\n",
      "hippocampus_203.nii.gz Dice 0.8984, Jaccard 0.8156. 57.69% complete\n",
      "hippocampus_056.nii.gz Dice 0.8964, Jaccard 0.8123. 61.54% complete\n",
      "hippocampus_130.nii.gz Dice 0.8938, Jaccard 0.8079. 65.38% complete\n",
      "hippocampus_125.nii.gz Dice 0.8893, Jaccard 0.8007. 69.23% complete\n",
      "hippocampus_301.nii.gz Dice 0.8641, Jaccard 0.7607. 73.08% complete\n",
      "hippocampus_217.nii.gz Dice 0.8160, Jaccard 0.6891. 76.92% complete\n",
      "hippocampus_199.nii.gz Dice 0.7861, Jaccard 0.6476. 80.77% complete\n",
      "hippocampus_350.nii.gz Dice 0.8681, Jaccard 0.7669. 84.62% complete\n",
      "hippocampus_227.nii.gz Dice 0.9194, Jaccard 0.8508. 88.46% complete\n",
      "hippocampus_083.nii.gz Dice 0.9021, Jaccard 0.8216. 92.31% complete\n",
      "hippocampus_341.nii.gz Dice 0.8644, Jaccard 0.7611. 96.15% complete\n",
      "hippocampus_330.nii.gz Dice 0.7473, Jaccard 0.5965. 100.00% complete\n",
      "mean Dice: 0.8772, mean Jaccard 0.7836.\n",
      "\n",
      "Testing complete.\n"
     ]
    }
   ],
   "source": [
    "# prep and run testing\n",
    "\n",
    "# TASK: Test method is not complete. Go to the method and complete it\n",
    "results_json = exp.run_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results_json[\"config\"] = vars(c)\n",
    "\n",
    "with open(os.path.join(exp.out_dir, 'results.json'), 'w') as out_file:\n",
    "    json.dump(results_json, out_file, indent=2, separators=(',', ': '))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'j' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3eedd8854d1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'j' is not defined"
     ]
    }
   ],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38pytorch",
   "language": "python",
   "name": "py38pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
